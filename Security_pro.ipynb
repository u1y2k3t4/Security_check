{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMvh9EpJvPeyInbvYSCg3ty",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/u1y2k3t4/Security_check/blob/main/Security_pro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "id": "deAqnuaHTiya",
        "outputId": "6f4b2c3e-44df-40cb-f12f-3b8f1a008466"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run `upload_and_run()` to upload a CSV and execute the security checks.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5114fc1b-4a2a-4822-b8ad-6db6ae02e298\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-5114fc1b-4a2a-4822-b8ad-6db6ae02e298\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving unsafe_dataset.csv to unsafe_dataset.csv\n",
            "Uploaded file: unsafe_dataset.csv\n",
            "\n",
            "=== DATASET SECURITY REPORT SUMMARY ===\n",
            "Overall score: 25/100\n",
            "Breakdown:\n",
            "The dataset is not safe\n",
            " - pii_score: 0/25\n",
            " - encryption_score: 0/25\n",
            " - poisoning_score: 0/25\n",
            " - integrity_score: 25/25\n",
            "\n",
            "Detailed findings saved to: /mnt/data/dataset_security_report/security_report_20250905T161849Z.json\n",
            "No flagged rows extracted. Inspect the full report for details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1538664279.py:410: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n",
            "/tmp/ipython-input-1538664279.py:466: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  timestamp = datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\")\n"
          ]
        }
      ],
      "source": [
        "# Colab-ready unified Dataset Security Scoring Pipeline\n",
        "# Handles: labelled datasets (with a target column), unlabelled datasets, and metadata\n",
        "# Outputs: overall score (0-100), per-check scores, detailed report, and CSVs with flagged rows.\n",
        "# Run this cell in Google Colab, then call upload_and_run()\n",
        "\n",
        "# --- Imports\n",
        "import io\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import hashlib\n",
        "from collections import Counter\n",
        "from datetime import datetime\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import entropy as scipy_entropy\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# --- Optional spaCy NER (used if available). If not present, pipeline will use regex only for PII detection.\n",
        "USE_SPACY = False\n",
        "try:\n",
        "    import spacy\n",
        "    try:\n",
        "        nlp = spacy.load(\"en_core_web_sm\")\n",
        "        USE_SPACY = True\n",
        "    except Exception as e:\n",
        "        # attempt to download model (works in Colab)\n",
        "        try:\n",
        "            print(\"Downloading spacy model...\")\n",
        "            os.system(\"python -m spacy download en_core_web_sm -q\")\n",
        "            nlp = spacy.load(\"en_core_web_sm\")\n",
        "            USE_SPACY = True\n",
        "        except Exception as e2:\n",
        "            print(\"spaCy model unavailable, continuing without NER. Reason:\", e2)\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"spaCy not installed; install with `pip install spacy` if you want NER support.\")\n",
        "\n",
        "# --- Helpers\n",
        "\n",
        "def sha256_of_file(path):\n",
        "    h = hashlib.sha256()\n",
        "    with open(path, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(8192), b\"\"):\n",
        "            h.update(chunk)\n",
        "    return h.hexdigest()\n",
        "\n",
        "def string_entropy(s: str):\n",
        "    if not s:\n",
        "        return 0.0\n",
        "    counts = Counter(s)\n",
        "    probs = np.array(list(counts.values())) / len(s)\n",
        "    return float(scipy_entropy(probs, base=2))\n",
        "\n",
        "def is_hash_like(s: str):\n",
        "    # simple checks for common hash lengths and base-16 hex characters (md5, sha1, sha256)\n",
        "    if not isinstance(s, str):\n",
        "        return False\n",
        "    s = s.strip()\n",
        "    hex_re = re.compile(r'^[0-9a-fA-F]+$')\n",
        "    if len(s) in (32, 40, 64) and hex_re.match(s):\n",
        "        return True\n",
        "    # bcrypt-ish pattern ($2b$...)\n",
        "    if s.startswith(\"$2a$\") or s.startswith(\"$2b$\") or s.startswith(\"$2y$\"):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "# --- PII patterns (extendable)\n",
        "PII_REGEX_PATTERNS = {\n",
        "    \"email\": re.compile(r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\"),\n",
        "    \"phone_10\": re.compile(r\"\\b\\d{10}\\b\"),\n",
        "    \"phone_international\": re.compile(r\"\\+?\\d[\\d \\-\\(\\)]{7,}\\d\"),\n",
        "    \"aadhaar\": re.compile(r\"\\b\\d{4}\\s?\\d{4}\\s?\\d{4}\\b\"),\n",
        "    \"ssn_like\": re.compile(r\"\\b\\d{3}-\\d{2}-\\d{4}\\b\"),\n",
        "    \"credit_card_like\": re.compile(r\"\\b(?:\\d[ -]*?){13,16}\\b\"),\n",
        "}\n",
        "\n",
        "# --- Main checks\n",
        "\n",
        "def pii_check(df, metadata=None, sample_frac=1.0, max_samples=20000):\n",
        "    \"\"\"\n",
        "    Detect PII in values and in metadata.\n",
        "    Returns: score (0-25), details dict\n",
        "    \"\"\"\n",
        "    # sample rows if dataset is huge\n",
        "    n = len(df)\n",
        "    if sample_frac < 1.0:\n",
        "        df_sample = df.sample(frac=sample_frac, random_state=42)\n",
        "    else:\n",
        "        df_sample = df if n <= max_samples else df.sample(n=max_samples, random_state=42)\n",
        "    findings = []\n",
        "    total_cells = 0\n",
        "    pii_cells = 0\n",
        "\n",
        "    # Check column names and metadata\n",
        "    meta_warnings = []\n",
        "    if metadata:\n",
        "        # metadata might be dict with keys like 'columns', 'description', 'source_hash'\n",
        "        for k,v in metadata.items():\n",
        "            text = str(v)\n",
        "            for name,pat in PII_REGEX_PATTERNS.items():\n",
        "                if pat.search(text):\n",
        "                    meta_warnings.append(f\"PII-like pattern in metadata field '{k}': {name}\")\n",
        "    for col in df.columns:\n",
        "        if re.search(r\"name|email|phone|ssn|aadhaar|id_card|passport\", col.lower()):\n",
        "            meta_warnings.append(f\"Suspicious column name: '{col}'\")\n",
        "    if meta_warnings:\n",
        "        findings.extend(meta_warnings)\n",
        "\n",
        "    # Check cell values (stringified)\n",
        "    text_cols = df_sample.select_dtypes(include=['object', 'string']).columns.tolist()\n",
        "    for col in text_cols:\n",
        "        for val in df_sample[col].astype(str).fillna(\"\"):\n",
        "            total_cells += 1\n",
        "            # regex checks\n",
        "            for name, pat in PII_REGEX_PATTERNS.items():\n",
        "                if pat.search(val):\n",
        "                    pii_cells += 1\n",
        "                    findings.append({\"col\": col, \"type\": name, \"value_sample\": val if len(val)<=200 else val[:200]})\n",
        "                    break\n",
        "            else:\n",
        "                # NER check if spaCy available\n",
        "                if USE_SPACY and val.strip():\n",
        "                    try:\n",
        "                        doc = nlp(val)\n",
        "                        for ent in doc.ents:\n",
        "                            if ent.label_ in (\"PERSON\", \"GPE\", \"LOC\", \"ORG\", \"DATE\"):\n",
        "                                pii_cells += 1\n",
        "                                findings.append({\"col\": col, \"type\": f\"ner_{ent.label_}\", \"value_sample\": val[:200]})\n",
        "                                break\n",
        "                    except Exception:\n",
        "                        pass\n",
        "\n",
        "    # scoring logic\n",
        "    if pii_cells == 0 and not findings:\n",
        "        score = 25\n",
        "    elif pii_cells <= max(1, 0.001 * total_cells):  # very rare PII in sampled cells\n",
        "        score = 15\n",
        "    else:\n",
        "        score = 0\n",
        "\n",
        "    details = {\n",
        "        \"total_text_cells_sampled\": int(total_cells),\n",
        "        \"pii_cells_detected\": int(pii_cells),\n",
        "        \"findings\": findings[:200]  # cap reported findings\n",
        "    }\n",
        "    return score, details\n",
        "\n",
        "def encryption_check(df, metadata=None, text_sample_size=200):\n",
        "    \"\"\"\n",
        "    Detect whether sensitive columns appear encrypted/hashed\n",
        "    Returns score (0-25) and details\n",
        "    \"\"\"\n",
        "    cols = df.columns.tolist()\n",
        "    encrypted_like_cols = []\n",
        "    suspicious_plain_cols = []\n",
        "    for col in cols:\n",
        "        # sample some values as strings\n",
        "        vals = df[col].dropna().astype(str).head(text_sample_size).tolist()\n",
        "        if not vals:\n",
        "            continue\n",
        "        entropies = [string_entropy(v) for v in vals]\n",
        "        avg_entropy = float(np.mean(entropies))\n",
        "        avg_len = float(np.mean([len(v) for v in vals]))\n",
        "        hash_likes = sum([1 for v in vals if is_hash_like(v)])\n",
        "        # heuristics:\n",
        "        # - Many hash-like values OR high avg_entropy and long length => likely encrypted/hashes\n",
        "        if hash_likes >= max(1, int(0.4*len(vals))) or (avg_entropy > 3.5 and avg_len > 20):\n",
        "            encrypted_like_cols.append({\"col\": col, \"avg_entropy\": avg_entropy, \"avg_len\": avg_len, \"hash_like_count\": int(hash_likes)})\n",
        "        else:\n",
        "            # if column name suggests sensitive and values are low entropy -> suspicious plain PII\n",
        "            if re.search(r\"pass|pwd|ssn|card|aadhar|aadhaar|email|secret|token\", col.lower()):\n",
        "                suspicious_plain_cols.append({\"col\": col, \"avg_entropy\": avg_entropy, \"avg_len\": avg_len})\n",
        "\n",
        "    # scoring heuristic\n",
        "    if not suspicious_plain_cols and encrypted_like_cols:\n",
        "        score = 25\n",
        "    elif suspicious_plain_cols and not encrypted_like_cols:\n",
        "        score = 0\n",
        "    elif encrypted_like_cols and suspicious_plain_cols:\n",
        "        score = 15\n",
        "    else:\n",
        "        # neither suspicious nor clearly encrypted: neutral\n",
        "        score = 15\n",
        "\n",
        "    details = {\n",
        "        \"encrypted_like_columns\": encrypted_like_cols,\n",
        "        \"suspicious_plain_columns\": suspicious_plain_cols\n",
        "    }\n",
        "    return score, details\n",
        "\n",
        "def poisoning_check(df, label_column=None, contamination=0.05, numeric_only=False):\n",
        "    \"\"\"\n",
        "    Detect poisoning/anomalies. Handles labelled and unlabelled datasets.\n",
        "    - If labelled: use NearestNeighbors label-consistency check to detect potential label flips.\n",
        "    - For numeric/unstructured: use IsolationForest on numeric features and TF-IDF embeddings for text columns.\n",
        "    Returns score (0-25) and details\n",
        "    \"\"\"\n",
        "    n = len(df)\n",
        "    # Prepare feature matrix\n",
        "    # Use numeric features + TF-IDF for text columns (sample/truncate for speed)\n",
        "    num_df = df.select_dtypes(include=[np.number]).copy()\n",
        "    text_cols = df.select_dtypes(include=['object', 'string']).columns.tolist()\n",
        "    # Drop obvious id columns\n",
        "    text_cols = [c for c in text_cols if not re.search(r'id$|^id|_id$', c.lower())]\n",
        "\n",
        "    # Build numeric matrix\n",
        "    X_num = None\n",
        "    if not num_df.empty:\n",
        "        X_num = num_df.fillna(0).values\n",
        "        scaler = StandardScaler()\n",
        "        X_num = scaler.fit_transform(X_num)\n",
        "\n",
        "    # Build text matrix via TF-IDF (concatenate text columns)\n",
        "    X_text = None\n",
        "    if text_cols:\n",
        "        sample_text = df[text_cols].astype(str).fillna(\" \").agg(\" \".join, axis=1).head(5000)  # cap for speed\n",
        "        tfidf = TfidfVectorizer(max_features=2000, stop_words='english')\n",
        "        try:\n",
        "            X_text = tfidf.fit_transform(sample_text).toarray()\n",
        "        except Exception as e:\n",
        "            X_text = None\n",
        "\n",
        "    # Combine features\n",
        "    if X_num is not None and X_text is not None:\n",
        "        # pad smaller one if needed\n",
        "        if X_num.shape[0] != X_text.shape[0]:\n",
        "            min_rows = min(X_num.shape[0], X_text.shape[0])\n",
        "            X_num = X_num[:min_rows]\n",
        "            X_text = X_text[:min_rows]\n",
        "        X = np.hstack([X_num[:X_text.shape[0]], X_text])\n",
        "    elif X_num is not None:\n",
        "        X = X_num\n",
        "    elif X_text is not None:\n",
        "        X = X_text\n",
        "    else:\n",
        "        # no usable features -> can't run poisoning detection; be optimistic\n",
        "        return 25, {\"reason\": \"No numeric or textual features to analyze for anomalies.\"}\n",
        "\n",
        "    # If labelled, run label-consistency KNN check\n",
        "    label_issues = []\n",
        "    if label_column and label_column in df.columns:\n",
        "        labels = df[label_column].astype(str).fillna(\"\")\n",
        "        # Need numeric/textual features aligned with labels; truncate to min size\n",
        "        m = min(len(labels), X.shape[0])\n",
        "        X_lab = X[:m]\n",
        "        labels = labels.values[:m]\n",
        "        # KNN neighbors\n",
        "        try:\n",
        "            knn = NearestNeighbors(n_neighbors=6, algorithm='auto').fit(X_lab)\n",
        "            dists, idxs = knn.kneighbors(X_lab, return_distance=True)\n",
        "            # For each sample, check fraction of neighbors with same label\n",
        "            inconsistent = 0\n",
        "            for i, neigh in enumerate(idxs):\n",
        "                neigh_labels = labels[neigh[1:]]  # ignore self at index 0\n",
        "                same = np.sum(neigh_labels == labels[i])\n",
        "                if same < 3:  # less than half neighbors agree -> suspicious label\n",
        "                    inconsistent += 1\n",
        "                    label_issues.append({\"index\": int(i), \"label\": labels[i], \"agreeing_neighbors\": int(same)})\n",
        "            fraction_inconsistent = inconsistent / m\n",
        "            if fraction_inconsistent == 0:\n",
        "                label_score = 25\n",
        "            elif fraction_inconsistent < 0.02:\n",
        "                label_score = 15\n",
        "            else:\n",
        "                label_score = 0\n",
        "        except Exception as e:\n",
        "            label_score = 15\n",
        "            label_issues.append({\"knn_error\": str(e)})\n",
        "    else:\n",
        "        label_score = None\n",
        "\n",
        "    # Isolation Forest on combined features\n",
        "    try:\n",
        "        iso = IsolationForest(contamination=contamination, random_state=42)\n",
        "        preds = iso.fit_predict(X)\n",
        "        anomalies = (preds == -1).sum()\n",
        "        frac_anom = anomalies / X.shape[0]\n",
        "        if frac_anom == 0:\n",
        "            iso_score = 25\n",
        "        elif frac_anom < contamination * 1.5:\n",
        "            iso_score = 15\n",
        "        else:\n",
        "            iso_score = 0\n",
        "    except Exception as e:\n",
        "        iso_score = 15\n",
        "        anomalies = None\n",
        "\n",
        "    # Combine scores: if labelled, consider both label_score and iso_score; else use iso_score\n",
        "    if label_score is not None:\n",
        "        # average out of 25\n",
        "        final_score = int(round((label_score + iso_score) / 2.0))\n",
        "    else:\n",
        "        final_score = iso_score\n",
        "\n",
        "    details = {\n",
        "        \"labelled_mode\": bool(label_column and label_column in df.columns),\n",
        "        \"label_check_score\": label_score,\n",
        "        \"label_issues_sample\": label_issues[:200],\n",
        "        \"isolation_forest_score\": iso_score,\n",
        "        \"anomalies_count\": int(anomalies) if anomalies is not None else None\n",
        "    }\n",
        "    return final_score, details\n",
        "\n",
        "def integrity_and_metadata_check(df, metadata=None):\n",
        "    \"\"\"\n",
        "    Checks schema consistency, missing values, duplicates, value ranges hints.\n",
        "    metadata can be dict containing expected schema, row_count, file_hash, column_types.\n",
        "    \"\"\"\n",
        "    issues = []\n",
        "    score = 25\n",
        "\n",
        "    # Missing values\n",
        "    missing_total = int(df.isnull().sum().sum())\n",
        "    if missing_total > 0:\n",
        "        issues.append(f\"{missing_total} missing values\")\n",
        "        score -= min(10, int(10 * (missing_total / max(1, df.size))))\n",
        "\n",
        "    # Duplicates\n",
        "    dup = int(df.duplicated().sum())\n",
        "    if dup > 0:\n",
        "        issues.append(f\"{dup} duplicate rows\")\n",
        "        score -= min(10, int(10 * (dup / max(1, len(df)))))\n",
        "\n",
        "    # Simple range sanity for numeric columns (detect extreme outliers beyond 5-sigma)\n",
        "    numeric = df.select_dtypes(include=[np.number])\n",
        "    extreme_vals = {}\n",
        "    if not numeric.empty:\n",
        "        for col in numeric.columns:\n",
        "            colvals = numeric[col].dropna()\n",
        "            if colvals.empty:\n",
        "                continue\n",
        "            mu = colvals.mean()\n",
        "            sigma = colvals.std()\n",
        "            if sigma == 0 or np.isnan(sigma):\n",
        "                continue\n",
        "            outliers = colvals[(colvals < mu - 5*sigma) | (colvals > mu + 5*sigma)]\n",
        "            if len(outliers) > 0:\n",
        "                extreme_vals[col] = int(len(outliers))\n",
        "    if extreme_vals:\n",
        "        issues.append(f\"Extreme-value columns: {extreme_vals}\")\n",
        "        score -= min(10, 5 * len(extreme_vals))\n",
        "\n",
        "    # Metadata checks (if provided)\n",
        "    meta_warnings = []\n",
        "    if metadata:\n",
        "        # verify expected row_count, columns, file_hash if present\n",
        "        if \"row_count\" in metadata and int(metadata[\"row_count\"]) != len(df):\n",
        "            meta_warnings.append(f\"Row count mismatch: metadata {metadata['row_count']} vs actual {len(df)}\")\n",
        "            score -= 5\n",
        "        if \"columns\" in metadata:\n",
        "            missing_cols = [c for c in metadata[\"columns\"] if c not in df.columns]\n",
        "            if missing_cols:\n",
        "                meta_warnings.append(f\"Missing columns from metadata: {missing_cols}\")\n",
        "                score -= 5\n",
        "        if \"file_hash\" in metadata and isinstance(metadata[\"file_hash\"], str):\n",
        "            # if metadata provides a file path or hash string; user may need to upload file to verify, skip if not matchable\n",
        "            try:\n",
        "                if \"file_path\" in metadata:\n",
        "                    if os.path.exists(metadata[\"file_path\"]):\n",
        "                        actual_hash = sha256_of_file(metadata[\"file_path\"])\n",
        "                        if actual_hash != metadata[\"file_hash\"]:\n",
        "                            meta_warnings.append(\"File hash mismatch vs metadata\")\n",
        "                            score -= 5\n",
        "                else:\n",
        "                    # cannot verify arbitrary hash without file\n",
        "                    pass\n",
        "            except Exception:\n",
        "                pass\n",
        "    if meta_warnings:\n",
        "        issues.extend(meta_warnings)\n",
        "\n",
        "    if score < 0:\n",
        "        score = 0\n",
        "\n",
        "    details = {\n",
        "        \"missing_total\": missing_total,\n",
        "        \"duplicates\": dup,\n",
        "        \"extreme_value_columns\": extreme_vals,\n",
        "        \"metadata_warnings\": meta_warnings,\n",
        "        \"issues\": issues\n",
        "    }\n",
        "    return score, details\n",
        "\n",
        "# --- Orchestration function ---\n",
        "\n",
        "def run_security_pipeline(df, metadata=None, label_column=None, contamination=0.05):\n",
        "    \"\"\"\n",
        "    Run all checks and produce a consolidated report.\n",
        "    \"\"\"\n",
        "    # 1. PII check\n",
        "    pii_score, pii_details = pii_check(df, metadata=metadata)\n",
        "\n",
        "    # 2. Encryption check\n",
        "    enc_score, enc_details = encryption_check(df, metadata=metadata)\n",
        "\n",
        "    # 3. Poisoning / anomaly check\n",
        "    poison_score, poison_details = poisoning_check(df, label_column=label_column, contamination=contamination)\n",
        "\n",
        "    # 4. Integrity & metadata check\n",
        "    integrity_score, integrity_details = integrity_and_metadata_check(df, metadata=metadata)\n",
        "\n",
        "    total_score = int(pii_score + enc_score + poison_score + integrity_score)\n",
        "\n",
        "    report = {\n",
        "        \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n",
        "        \"total_score\": total_score,\n",
        "        \"breakdown\": {\n",
        "            \"pii_score\": pii_score,\n",
        "            \"encryption_score\": enc_score,\n",
        "            \"poisoning_score\": poison_score,\n",
        "            \"integrity_score\": integrity_score\n",
        "        },\n",
        "        \"details\": {\n",
        "            \"pii\": pii_details,\n",
        "            \"encryption\": enc_details,\n",
        "            \"poisoning\": poison_details,\n",
        "            \"integrity\": integrity_details\n",
        "        }\n",
        "    }\n",
        "    return report\n",
        "\n",
        "# --- Colab-friendly upload + run ---\n",
        "\n",
        "def upload_and_run():\n",
        "    try:\n",
        "        from google.colab import files\n",
        "        uploaded = files.upload()\n",
        "        if not uploaded:\n",
        "            print(\"No file uploaded.\")\n",
        "            return None, None\n",
        "        fname = list(uploaded.keys())[0]\n",
        "        print(\"Uploaded file:\", fname)\n",
        "        df = pd.read_csv(fname)\n",
        "        # Optional: look for metadata file with same base name + .meta.json if user uploads\n",
        "        metadata = {}\n",
        "        meta_name = fname.rsplit(\".\",1)[0] + \".meta.json\"\n",
        "        if meta_name in uploaded:\n",
        "            try:\n",
        "                metadata = json.loads(uploaded[meta_name].decode('utf-8'))\n",
        "            except Exception:\n",
        "                pass\n",
        "    except Exception:\n",
        "        # fallback for non-Colab: try loading a local path\n",
        "        fname = input(\"Enter local CSV path: \").strip()\n",
        "        df = pd.read_csv(fname)\n",
        "        metadata = None\n",
        "\n",
        "    # auto-detect label column heuristically if not provided\n",
        "    label_col = None\n",
        "    for candidate in [\"label\", \"target\", \"class\", \"y\"]:\n",
        "        if candidate in df.columns:\n",
        "            label_col = candidate\n",
        "            break\n",
        "\n",
        "    # run pipeline\n",
        "    report = run_security_pipeline(df, metadata=metadata, label_column=label_col, contamination=0.05)\n",
        "\n",
        "    # save report + flagged items\n",
        "    out_dir = \"/mnt/data/dataset_security_report\"\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    timestamp = datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\")\n",
        "    report_path = os.path.join(out_dir, f\"security_report_{timestamp}.json\")\n",
        "    with open(report_path, \"w\") as f:\n",
        "        json.dump(report, f, indent=2)\n",
        "\n",
        "    # Attempt to save flagged rows (PII and anomalies) for user review\n",
        "    flagged_rows = pd.DataFrame()\n",
        "    try:\n",
        "        # PII flagged rows\n",
        "        pii_findings = report[\"details\"][\"pii\"][\"findings\"]\n",
        "        if pii_findings:\n",
        "            rows = []\n",
        "            for fnd in pii_findings:\n",
        "                col = fnd.get(\"col\")\n",
        "                sample_val = fnd.get(\"value_sample\")\n",
        "                if col and sample_val is not None:\n",
        "                    # find rows where col contains the sample snippet\n",
        "                    mask = df[col].astype(str).str.contains(re.escape(sample_val[:50]), na=False)\n",
        "                    rows.append(df[mask])\n",
        "            if rows:\n",
        "                flagged_rows = pd.concat(rows).drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "\n",
        "    flagged_path = None\n",
        "    if not flagged_rows.empty:\n",
        "        flagged_path = os.path.join(out_dir, f\"flagged_rows_{timestamp}.csv\")\n",
        "        flagged_rows.to_csv(flagged_path, index=False)\n",
        "\n",
        "\n",
        "    print(\"\\n=== DATASET SECURITY REPORT SUMMARY ===\")\n",
        "    print(f\"Overall score: {report['total_score']}/100\")\n",
        "    print(\"Breakdown:\")\n",
        "    if report['total_score'] >= 80:\n",
        "      print(\"The dataset is safe\")\n",
        "    elif report['total_score'] > 50 and report['total_score']  < 80:\n",
        "      print(\"The dataset has some security issues\")\n",
        "    else:\n",
        "      print(\"The dataset is not safe\")\n",
        "    for k,v in report[\"breakdown\"].items():\n",
        "        print(f\" - {k}: {v}/25\")\n",
        "    print(\"\\nDetailed findings saved to:\", report_path)\n",
        "    if flagged_path:\n",
        "        print(\"Flagged rows saved to:\", flagged_path)\n",
        "    else:\n",
        "        print(\"No flagged rows extracted. Inspect the full report for details.\")\n",
        "\n",
        "    return report, {\"report_path\": report_path, \"flagged_path\": flagged_path}\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Run `upload_and_run()` to upload a CSV and execute the security checks.\")\n",
        "report, paths = upload_and_run()\n"
      ]
    }
  ]
}